{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Neuroimaging in Python\n",
    "\n",
    "For a good intro to the jupyter notebook interface and general python check out the following web tutorials.\n",
    "https://miykael.github.io/nipype_tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit and run this chunck if you are using your own laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use a single subject from the ABIDE dataset.\n",
    "import os\n",
    "## edit this section if you are running this one your own laptop\n",
    "tutorial_dir ='/home/USERNAME/Documents/ss2018_pythonmri/'  ## point this to the location where you want the data\n",
    "nilearn_data = os.path.join(tutorial_dir, 'data', 'nilearn_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you are running this on the SciNet Jyputer Hub Run the next section..\n",
    "\n",
    "If not you can skip down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if the \"coss2018_mri\" environment not available\n",
    "\n",
    "Run the next block to link the built conda env into your own home. (You might need to restart the kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ~/conda/envs\n",
    "! ln -s /scinet/course/ss2018/3_bm/1_mripython/conda_envs/coss2018_mri ~/.conda/envs/coss2018_mri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use a single subject from the ABIDE dataset.\n",
    "import os\n",
    "\n",
    "# # ## use these commands if you are running this on scinet\n",
    "my_home = os.environ.get('HOME')\n",
    "my_scratch = my_home.replace('/home/', '/scratch/')\n",
    "tutorial_dir = os.path.join(my_scratch, 'coss2018_mripython')\n",
    "\n",
    "# make the tutorial dir if it does not exist\n",
    "if not os.path.exists(tutorial_dir):\n",
    "    os.makedirs(tutorial_dir)\n",
    "\n",
    "# we are all going to read from the data in the courses folder (on the burst buffer!)\n",
    "nilearn_data = '/scinet/course/ss2018/3_bm/1_mripython/data/nilearn_data'\n",
    "\n",
    "cc200_nii = os.path.join(os.path.dirname(os.getcwd()), 'rois', 'cc200_roi_atlas.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make the output directories if they don't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc200_nii = os.path.join(os.path.dirname(os.getcwd()), 'rois', 'cc200_roi_atlas.nii.gz')\n",
    "    \n",
    "## if this directory doesn't exist - make it..\n",
    "rois_dir = os.path.join(tutorial_dir, 'data', 'rois')\n",
    "if not os.path.exists(rois_dir):\n",
    "    os.makedirs(rois_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nilearn data fetchers to grab our tutorial dataset\n",
    "\n",
    "We are grabbing data from the ABIDE dataset. A data sharing initiative with over 1000 scans. Check out the references included in the abide object for more info. \n",
    "\n",
    "Thanks to the work of the proprocessed connectome project. A lot of the data processing work has been done for us! We are going to grab the preprocessed but unfilted resting stat MR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "\n",
    "# We specify the site and number of subjects we want to download\n",
    "adhd = datasets.fetch_adhd(data_dir=nilearn_data, \n",
    "                                 n_subjects = 3)\n",
    "\n",
    "print(adhd.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{}'.format(adhd.description.decode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept Question 1:\n",
    "\n",
    "Explore the adhd object. What is the path to the functional files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept Question 2:\n",
    "\n",
    "How was the data processed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adhd.phenotypic[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we will define the files we want to work with for the rest of the tutorial \n",
    "\n",
    "We need:\n",
    "+ some Regions of Interest (rois) to use as masks for our analysis\n",
    "+ a functional MR image\n",
    "+ (for plotting) The mean of our functional MR image\n",
    "+ (for masking) a mask defining the brain vs outside of head\n",
    "+ to decide which roi we will focus on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = cc200_nii\n",
    "func = adhd.func[0]\n",
    "func_confounds = adhd.confounds[0]\n",
    "roi = 174"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remember this line..it is magik!!! and important for seeing plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes the plots show up..\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "cc200_nii\n",
    "plotting.plot_roi(cc200_nii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image as img\n",
    "\n",
    "mean_img = img.mean_img(func)\n",
    "\n",
    "plotting.plot_epi(mean_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_roi(cc200_nii, bg_img=mean_img, alpha = 0.5, title = \"cc200 atlas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nibabel to load nifti images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for interacting with NIFTI files\n",
    "import nibabel as nib\n",
    "\n",
    "func_nib = nib.load(func)\n",
    "print(func_nib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We extract the affine matrix (we will use this to make a new image\n",
    "\n",
    "_Note_: in older version of nibabel the syntax is `affine = func_nib.get_affine()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine = func_nib.affine\n",
    "affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = func_nib.header\n",
    "print(header.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dimensions for the fMRI file\n",
    "dims = func_nib.shape\n",
    "dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use get data to extract the data from it\n",
    "func_data = func_nib.get_data()\n",
    "func_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkling that our functional and mask files have the same dimensions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the same thing for rois\n",
    "rois_data = nib.load(cc200_nii).get_data()\n",
    "\n",
    "print(rois_data.shape)\n",
    "\n",
    "if rois_data.shape[0:2] == dims[0:2]:\n",
    "    print(\"ROIs and func file dimensions match, Hooray!!\")\n",
    "else:\n",
    "    print(\"FAIL, they do not match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using nilearn to sample the rois nifti file so that they match in dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_cc200 = img.resample_to_img(cc200_nii, mean_img, interpolation = 'nearest')\n",
    "\n",
    "plotting.plot_roi(resampled_cc200, mean_img)\n",
    "\n",
    "resampled_cc200.to_filename(os.path.join(rois_dir, \"resample_cc200.nii.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the same thing for rois\n",
    "new_rois = os.path.join(tutorial_dir, 'data', 'rois', \"resample_cc200.nii.gz\")\n",
    "rois_data = nib.load(new_rois).get_data()\n",
    "\n",
    "print(rois_data.shape)\n",
    "\n",
    "if rois_data.shape[0:2] == dims[0:2]:\n",
    "    print(\"ROIs and func file dimensions match, Hooray!!\")\n",
    "else:\n",
    "    print(\"FAIL, they do not match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating our own seed correlation map using numpy\n",
    "\n",
    "**Step 1:** To make our math easier, we will reshape our data from 4D to 2D\n",
    "\n",
    "**Now :**\n",
    "\n",
    "+ axis 0 (rows): space (all of the voxels stacked)\n",
    "+ axis 1 (columns): time (timepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape to voxels * timepoints (4D --> 2D)\n",
    "func_data = func_data.reshape(dims[0]*dims[1]*dims[2], dims[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois_data = rois_data.reshape(dims[0]*dims[1]*dims[2], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating our mean timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# get the seed time series\n",
    "idx = np.where(rois_data == roi)[0]\n",
    "\n",
    "# look at the timeseries\n",
    "sns.tsplot(func_data[idx, :], err_style=\"unit_traces\")\n",
    "\n",
    "len(ts) # looks like it's the right length (i.e., our axis is correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.mean(func_data[idx, :], axis=0)\n",
    "\n",
    "# look at the timeseries\n",
    "sns.tsplot(ts)\n",
    "\n",
    "len(ts) # looks like it's the right length (i.e., our axis is correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlate the mean timeseries with the timeseries from every voxel in the func file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an output matrix\n",
    "output = np.zeros(dims[0]*dims[1]*dims[2])\n",
    "\n",
    "# correlate seed against all voxels\n",
    "for i in range(dims[0]*dims[1]*dims[2]):\n",
    "    output[i] = np.corrcoef(ts, func_data[i, :])[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When will do a faster and with no \"true divide\" warning if we incorporate the brain mask\n",
    "\n",
    "first lets look at our main img to make one.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_img_reshaped = mean_img.get_data().reshape(dims[0]*dims[1]*dims[2],1)\n",
    "np.unique(mean_img_reshaped)[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add a mask to get rid of the weird bits\n",
    "mask_data = np.where(mean_img_reshaped > 1)[0]\n",
    "idx_mask = np.where(mask_data > 0)[0]\n",
    "\n",
    "## recalculate the ts taking the mask into account\n",
    "idx_masked = np.intersect1d(idx,idx_mask)\n",
    "ts = np.mean(func_data[idx_masked, :], axis=0)\n",
    "\n",
    "# make an output matrix\n",
    "output = np.zeros(dims[0]*dims[1]*dims[2])\n",
    "\n",
    "# correlate seed against all voxels\n",
    "for i in np.arange(len(idx_mask)):\n",
    "    output[idx_mask[i]] = np.corrcoef(ts, func_data[idx_mask[i], :])[0][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nibabel to write our result out to a nifti file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get back to 4D\n",
    "output_filename = os.path.join(tutorial_dir, 'seed_correlation.nii.gz')\n",
    "output = np.reshape(output, (dims[0], dims[1], dims[2], 1))\n",
    "\n",
    "# write the results into a NIFTI file\n",
    "output = nib.nifti1.Nifti1Image(output, affine)\n",
    "output.to_filename(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nilearn's glass brain and statmap plots help us visualize our new result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_glass_brain(output_filename, threshold = 0.1, \n",
    "                          colorbar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(output_filename, threshold = 0.3, \n",
    "                          colorbar = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instead of calculating one mean timeseries, let's do it for the whole atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output matrix\n",
    "output_ts = np.zeros((len(np.unique(rois_data))-1, dims[3]))\n",
    "\n",
    "# rois_data_masked = np.multiply(rois_data,mask_data)\n",
    "# load in all mean time series from atlas\n",
    "for i, roi in enumerate(np.unique(rois_data)):\n",
    "    if roi > 0:\n",
    "        idx = np.where(rois_data == roi)[0]\n",
    "        ts = np.mean(func_data[idx, :], axis=0)\n",
    "        output_ts[i-1, :] = ts\n",
    "\n",
    "# correlation matrix\n",
    "cc200_correl = np.corrcoef(output_ts)\n",
    "\n",
    "# view output\n",
    "plt.imshow(cc200_correl, interpolation='nearest', cmap=plt.cm.RdBu_r)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The seaborn package has some really awesome option..uncluding and easy clustered heatmap plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "\n",
    "seaborn.clustermap(cc200_correl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting fancier with nilearn\n",
    "\n",
    "nilearn was built by people doing machine learning of resting state fMRI data using sci-kit learn.\n",
    "\n",
    "It has A LOT of functionallity beyond what we have time to teach here. Including:\n",
    "+ image manipulation (smoothing, resampling)\n",
    "+ algorithms (ICA and dictionary learning) for parcellating your own dataset (i.e. defining your own ROIS) \n",
    "+ timeseries extraction with many many options for filtering, detrending \n",
    "+ multiple methods for covariance esimtaiton (i.e. partial correlation, covariance, lasso..)\n",
    "+ machine learning and cross validation\n",
    "\n",
    "They also did a beautiful job of building a large set of ipython notebook tutorials to teach you about all these methods.\n",
    "\n",
    "Go to the [the nilean website](http://nilearn.github.io/index.html) for more info.\n",
    "\n",
    "Let's use another example to see some of the more complex things nilearn can do...\n",
    "\n",
    "We'll start by using the data fetcher to grab a different atlas. This one (dosenbach_2010) if a set of coordinates in the brain to sample from.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "\n",
    "dosenbach = datasets.fetch_coords_dosenbach_2010()\n",
    "print(dosenbach.description.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using nilearn's sphere masker to extract the timeseries\n",
    "\n",
    "nilearn has a built in function for extracting timeseries from functional files and doing a little extra signal processing at the same time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import input_data\n",
    "\n",
    "spheres_masker = input_data.NiftiSpheresMasker(\n",
    "    seeds=dosenbach.rois, #the seeds are the dosenbach roi atlas\n",
    "    smoothing_fwhm=4, radius=4.5, # set the radius of a sphere around the roi you want extracted\n",
    "    standardize=True, # the time-series are centered and normed (mean 0, variance 1 in the time dimension)\n",
    "    detrend=True, low_pass=0.1, high_pass=0.01, t_r=2.5, confound = adhd.confounds[0]) # additional signal cleaning and filtering params\n",
    "\n",
    "timeseries = spheres_masker.fit_transform(func)\n",
    "\n",
    "print(\"the shape of the timeseries is {}\".format(timeseries.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using nilearn's ConnectivityMeasure to calculate our correlation matrix\n",
    "\n",
    "Avalable options are “correlation”, “partial correlation”, “tangent”, “covariance”, “precision” or other utilites in sci-py could be plugged in ([see here for an example](http://nilearn.github.io/auto_examples/03_connectivity/plot_multi_subject_connectome.html#sphx-glr-auto-examples-03-connectivity-plot-multi-subject-connectome-py))\n",
    "\n",
    "Let's do partial correlation this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn import plotting\n",
    "\n",
    "correlation_measure = ConnectivityMeasure(kind='covariance')\n",
    "dosenbach_matrix = correlation_measure.fit_transform([timeseries])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.clustermap(dosenbach_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "\n",
    "coords = np.vstack((\n",
    "    dosenbach.rois['x'],\n",
    "    dosenbach.rois['y'],\n",
    "    dosenbach.rois['z'],\n",
    ")).T\n",
    "\n",
    "plotting.plot_connectome(dosenbach_matrix, node_coords = coords, edge_threshold='98%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Now let's repeat the sphere's with confound cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import input_data\n",
    "\n",
    "spheres_masker = input_data.NiftiSpheresMasker(\n",
    "    seeds=dosenbach.rois, #the seeds are the dosenbach roi atlas\n",
    "    smoothing_fwhm=4, radius=4.5, # set the radius of a sphere around the roi you want extracted\n",
    "    standardize=True, # the time-series are centered and normed (mean 0, variance 1 in the time dimension)\n",
    "    detrend=True, low_pass=0.1, high_pass=0.01, t_r=2.5) # additional signal cleaning and filtering params\n",
    "\n",
    "timeseries = spheres_masker.fit_transform(func, confounds=func_confounds)\n",
    "\n",
    "print(\"the shape of the timeseries is {}\".format(timeseries.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_measure = ConnectivityMeasure(kind='covariance')\n",
    "dosenbach_matrix = correlation_measure.fit_transform([timeseries])[0]\n",
    "seaborn.clustermap(dosenbach_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now let's try cleaning the whole image with image.clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image clearning inside nilearn\n",
    "\n",
    "cleaned_func = img.clean_img(func, confounds=func_confounds, low_pass=0.1, high_pass=0.01, t_r=2.0,)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [coss2018_mri]",
   "language": "python",
   "name": "conda-env-coss2018_mri-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
